# Assignment 3  
## Архитектура GPU и оптимизация CUDA-программ

---

## Цель работы

Целью данного задания является изучение архитектуры GPU и методов оптимизации CUDA-программ, включая использование различных типов памяти, влияние конфигурации блоков потоков и особенности доступа к глобальной памяти.

---

## Среда выполнения

- Среда: Google Colab  
- GPU: NVIDIA Tesla T4  
- Язык программирования: CUDA C++  
- Компиляция: nvcc с указанием архитектуры `sm_75`

---

## Задание 1. Поэлементная обработка массива

### Описание

Реализованы две версии CUDA-программы для поэлементного умножения элементов массива:
1. Реализация с использованием только глобальной памяти.
2. Реализация с использованием разделяемой памяти.

Размер массива — 1 000 000 элементов. Выполнено сравнение времени выполнения обеих реализаций.

### Результаты выполнения

![Output задания 1](output1.png)

### Диаграмма алгоритма

![Диаграмма задания 1](diagram1_ass3.png)

### Вывод

Использование разделяемой памяти позволяет сократить количество обращений к глобальной памяти и повысить производительность по сравнению с реализацией, использующей только глобальную память.

---

## Задание 2. Влияние размера блока потоков

### Описание

Реализована CUDA-программа для поэлементного сложения двух массивов. Проведены замеры времени выполнения для различных размеров блока потоков (например, 128, 256 и 512 потоков).

### Результаты выполнения

![Output задания 2](output2.png)

### Диаграмма алгоритма

![Диаграмма задания 2](diagram2_ass3.png)

### Вывод

Размер блока потоков оказывает существенное влияние на производительность CUDA-программы. Оптимальный размер блока обеспечивает более эффективную загрузку вычислительных ресурсов GPU.

---

## Задание 3. Доступ к глобальной памяти

### Описание

Реализованы две версии CUDA-программы:
- с коалесцированным доступом к глобальной памяти;
- с некоалесцированным доступом к глобальной памяти.

Проведено сравнение времени выполнения для массива размером 1 000 000 элементов.

### Результаты выполнения

![Output задания 3](output3.png)

### Диаграмма алгоритма

![Диаграмма задания 3](diagram3_ass3.png)

### Вывод

Коалесцированный доступ к глобальной памяти обеспечивает более высокую производительность за счёт уменьшения числа транзакций памяти по сравнению с некоалесцированным доступом.

---

## Задание 4. Оптимизация конфигурации сетки и блоков

### Описание

Для одной из реализованных CUDA-программ была выполнена оптимизация параметров конфигурации сетки и блоков потоков. Сравнивалась производительность неоптимальной и оптимизированной конфигураций.

### Результаты выполнения

![Output задания 4](output4.png)

### Диаграмма алгоритма

![Диаграмма задания 4](diagram4_ass3.png)

### Вывод

Оптимальный выбор конфигурации сетки и блоков потоков позволяет значительно повысить производительность CUDA-программы за счёт более эффективного использования ресурсов GPU.

---

## Общие выводы

В ходе выполнения Assignment 3 были изучены ключевые аспекты архитектуры GPU и оптимизации CUDA-программ. Эксперименты показали, что использование разделяемой памяти, коалесцированного доступа к глобальной памяти и корректной конфигурации блоков потоков существенно влияет на производительность параллельных вычислений.

---
Контрольные вопросы к Assignment 3

(CUDA и архитектура GPU)

1. Какие основные типы памяти существуют в архитектуре CUDA и чем они отличаются по скорости доступа?

В архитектуре CUDA используются глобальная, разделяемая, локальная, константная и регистровая память. Регистры и разделяемая память имеют наименьшую задержку доступа и используются внутри потоков и блоков. Глобальная память обладает наибольшей задержкой, но доступна всем потокам и имеет большой объём. Константная память оптимизирована для чтения и используется при неизменяемых данных.

2. В каких случаях использование разделяемой памяти позволяет ускорить выполнение CUDA-программы?

Разделяемая память позволяет ускорить выполнение программы в случаях, когда данные многократно используются потоками одного блока. Перемещение данных из глобальной памяти в разделяемую снижает количество медленных обращений к глобальной памяти и уменьшает задержки доступа.

3. Как шаблон доступа к глобальной памяти влияет на производительность GPU-программы?

Шаблон доступа к глобальной памяти напрямую влияет на эффективность использования пропускной способности памяти. Коалесцированный доступ, при котором соседние потоки обращаются к соседним адресам памяти, обеспечивает высокую производительность. Некоалесцированный доступ приводит к увеличению числа транзакций памяти и снижению производительности.

4. Почему одинаковый алгоритм на GPU может показывать разное время выполнения при разных способах обращения к памяти?

Разное время выполнения связано с различной эффективностью использования памяти. Даже при одинаковом алгоритме неэффективный доступ к глобальной памяти может значительно увеличить задержки и снизить пропускную способность, что приводит к ухудшению общей производительности.

5. Как размер блока потоков влияет на производительность CUDA-ядра?

Размер блока потоков определяет количество потоков, выполняемых одновременно на мультипроцессоре GPU. Слишком маленький блок приводит к недостаточной загрузке вычислительных ресурсов, а слишком большой — к ограничению по доступным регистрам и разделяемой памяти. Оптимальный размер блока позволяет максимально эффективно использовать ресурсы GPU.

6. Что такое варп и почему важно учитывать его при разработке CUDA-программ?

Варп — это группа из 32 потоков, которые выполняются синхронно на GPU. Важно учитывать варпы, так как расхождение ветвлений внутри варпа (warp divergence) приводит к последовательному выполнению веток и снижению производительности.

7. Какие факторы необходимо учитывать при выборе конфигурации сетки и блоков потоков?

При выборе конфигурации необходимо учитывать архитектуру GPU, размер задачи, использование регистров и разделяемой памяти, а также требования к коалесцированному доступу к памяти. Целью является достижение максимальной загрузки вычислительных ресурсов при минимальных накладных расходах.

8. Почему оптимизация CUDA-программы часто начинается с анализа работы с памятью, а не с изменения алгоритма?

Во многих CUDA-программах производительность ограничена пропускной способностью памяти, а не вычислительными возможностями GPU. Оптимизация доступа к памяти, уменьшение количества обращений к глобальной памяти и использование разделяемой памяти часто дают больший прирост производительности, чем изменение самого алгоритма.