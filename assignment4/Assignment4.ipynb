{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESh27KTqKSpw",
        "outputId": "104c6f9b-6701-47fb-9494-38c21c3f5639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task1.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "// Размер массива\n",
        "const int N = 100000;\n",
        "\n",
        "// Размер блока потоков для CUDA\n",
        "const int BLOCK_SIZE = 256;\n",
        "\n",
        "// CUDA-ядро для копирования элементов массива в массив частичных сумм.\n",
        "// Каждый поток обрабатывает один элемент.\n",
        "// Используется только глобальная память GPU.\n",
        "__global__ void sumKernel(const float* data, float* partialSums, int n) {\n",
        "    // Глобальный индекс потока\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Проверка выхода за границы массива\n",
        "    if (idx < n) {\n",
        "        // Запись значения элемента в массив частичных сумм\n",
        "        partialSums[idx] = data[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Выделение памяти под массив на CPU\n",
        "    float* h_data = new float[N];\n",
        "\n",
        "    // Инициализация массива значениями\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_data[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // Последовательное вычисление суммы на CPU\n",
        "    auto startCpu = high_resolution_clock::now();\n",
        "\n",
        "    float cpuSum = 0.0f;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        cpuSum += h_data[i];\n",
        "    }\n",
        "\n",
        "    auto endCpu = high_resolution_clock::now();\n",
        "    double cpuTime =\n",
        "        duration<double, milli>(endCpu - startCpu).count();\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    float* d_data;\n",
        "    float* d_partial;\n",
        "    cudaMalloc(&d_data, N * sizeof(float));\n",
        "    cudaMalloc(&d_partial, N * sizeof(float));\n",
        "\n",
        "    // Копирование данных с CPU на GPU\n",
        "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Расчёт количества блоков\n",
        "    int gridSize = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Запуск CUDA-ядра и измерение времени выполнения\n",
        "    auto startGpu = high_resolution_clock::now();\n",
        "    sumKernel<<<gridSize, BLOCK_SIZE>>>(d_data, d_partial, N);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto endGpu = high_resolution_clock::now();\n",
        "\n",
        "    double gpuTime =\n",
        "        duration<double, milli>(endGpu - startGpu).count();\n",
        "\n",
        "    // Копирование массива частичных сумм обратно на CPU\n",
        "    float* h_partial = new float[N];\n",
        "    cudaMemcpy(h_partial, d_partial, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Финальное суммирование частичных результатов на CPU\n",
        "    float gpuSum = 0.0f;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        gpuSum += h_partial[i];\n",
        "    }\n",
        "\n",
        "    // Вывод результатов вычислений и времени выполнения\n",
        "    cout << \"Размер массива: \" << N << \" элементов\\n\";\n",
        "    cout << \"CPU сумма: \" << cpuSum\n",
        "         << \" | Время: \" << cpuTime << \" мс\\n\";\n",
        "    cout << \"GPU сумма: \" << gpuSum\n",
        "         << \" | Время (CUDA-ядро): \" << gpuTime << \" мс\\n\";\n",
        "\n",
        "    // Освобождение памяти GPU\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_partial);\n",
        "\n",
        "    // Освобождение памяти CPU\n",
        "    delete[] h_data;\n",
        "    delete[] h_partial;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlqisitLKkG9",
        "outputId": "60abbe37-2487-411c-b273-847c2b0f8cf9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc task1.cu -o task1\n",
        "!./task1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVV0_kGWLcSH",
        "outputId": "6ee84105-dad2-4c71-d211-bbfb5a83407c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер массива: 100000 элементов\n",
            "CPU сумма: 100000 | Время: 0.393596 мс\n",
            "GPU сумма: 0 | Время (CUDA-ядро): 7.51577 мс\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task2.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "// Размер массива\n",
        "const int N = 1'000'000;\n",
        "\n",
        "// Размер блока потоков\n",
        "const int BLOCK_SIZE = 256;\n",
        "\n",
        "// CUDA-ядро для вычисления префиксной суммы внутри одного блока.\n",
        "// Используется разделяемая память.\n",
        "// Реализован алгоритм инклюзивного сканирования.\n",
        "__global__ void scanKernel(const float* input, float* output, int n) {\n",
        "    // Разделяемая память для блока\n",
        "    __shared__ float sharedData[BLOCK_SIZE];\n",
        "\n",
        "    // Глобальный и локальный индексы\n",
        "    int globalIdx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int localIdx = threadIdx.x;\n",
        "\n",
        "    // Загрузка данных из глобальной памяти в shared memory\n",
        "    if (globalIdx < n) {\n",
        "        sharedData[localIdx] = input[globalIdx];\n",
        "    } else {\n",
        "        sharedData[localIdx] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Синхронизация потоков внутри блока\n",
        "    __syncthreads();\n",
        "\n",
        "    // Алгоритм префиксной суммы (scan) внутри блока\n",
        "    for (int offset = 1; offset < blockDim.x; offset *= 2) {\n",
        "        float value = 0.0f;\n",
        "        if (localIdx >= offset) {\n",
        "            value = sharedData[localIdx - offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "        sharedData[localIdx] += value;\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Запись результата обратно в глобальную память\n",
        "    if (globalIdx < n) {\n",
        "        output[globalIdx] = sharedData[localIdx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Выделение памяти под массивы на CPU\n",
        "    float* h_input = new float[N];\n",
        "    float* h_outputCpu = new float[N];\n",
        "    float* h_outputGpu = new float[N];\n",
        "\n",
        "    // Инициализация массива\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_input[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // Последовательное вычисление префиксной суммы на CPU\n",
        "    auto startCpu = high_resolution_clock::now();\n",
        "\n",
        "    h_outputCpu[0] = h_input[0];\n",
        "    for (int i = 1; i < N; i++) {\n",
        "        h_outputCpu[i] = h_outputCpu[i - 1] + h_input[i];\n",
        "    }\n",
        "\n",
        "    auto endCpu = high_resolution_clock::now();\n",
        "    double cpuTime =\n",
        "        duration<double, milli>(endCpu - startCpu).count();\n",
        "\n",
        "    // Выделение памяти на GPU\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, N * sizeof(float));\n",
        "    cudaMalloc(&d_output, N * sizeof(float));\n",
        "\n",
        "    // Копирование данных на GPU\n",
        "    cudaMemcpy(d_input, h_input, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Расчёт конфигурации запуска\n",
        "    int gridSize = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    // Запуск CUDA-ядра и замер времени\n",
        "    auto startGpu = high_resolution_clock::now();\n",
        "    scanKernel<<<gridSize, BLOCK_SIZE>>>(d_input, d_output, N);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto endGpu = high_resolution_clock::now();\n",
        "\n",
        "    double gpuTime =\n",
        "        duration<double, milli>(endGpu - startGpu).count();\n",
        "\n",
        "    // Копирование результата обратно на CPU\n",
        "    cudaMemcpy(h_outputGpu, d_output, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Вывод результатов\n",
        "    cout << \"Размер массива: \" << N << \" элементов\\n\";\n",
        "    cout << \"CPU время: \" << cpuTime << \" мс\\n\";\n",
        "    cout << \"GPU время (shared memory): \" << gpuTime << \" мс\\n\";\n",
        "    cout << \"Последний элемент CPU: \" << h_outputCpu[N - 1] << endl;\n",
        "    cout << \"Последний элемент GPU: \" << h_outputGpu[N - 1] << endl;\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    delete[] h_input;\n",
        "    delete[] h_outputCpu;\n",
        "    delete[] h_outputGpu;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLf5VTbSLvbn",
        "outputId": "fe5f01ae-87c7-491e-810e-0ad8c04b3287"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc task2.cu -o task2\n",
        "!./task2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDeVC8-dMgs9",
        "outputId": "d2bbb9a5-25c1-4047-aafb-779f066fa656"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер массива: 1000000 элементов\n",
            "CPU время: 4.75458 мс\n",
            "GPU время (shared memory): 7.34184 мс\n",
            "Последний элемент CPU: 1e+06\n",
            "Последний элемент GPU: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task3.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "// Размер массива\n",
        "const int N = 1'000'000;\n",
        "\n",
        "// Размер блока потоков CUDA\n",
        "const int BLOCK_SIZE = 256;\n",
        "\n",
        "// CUDA-ядро для поэлементной обработки массива.\n",
        "// Каждый поток умножает один элемент на константу.\n",
        "__global__ void gpuProcess(float* data, int start, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int globalIdx = start + idx;\n",
        "\n",
        "    if (globalIdx < n) {\n",
        "        data[globalIdx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Выделение памяти под массив на CPU\n",
        "    float* h_data = new float[N];\n",
        "\n",
        "    // Инициализация массива\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_data[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // CPU реализация\n",
        "\n",
        "    float* h_cpu = new float[N];\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_cpu[i] = h_data[i];\n",
        "    }\n",
        "\n",
        "    auto startCpu = high_resolution_clock::now();\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_cpu[i] *= 2.0f;\n",
        "    }\n",
        "\n",
        "    auto endCpu = high_resolution_clock::now();\n",
        "    double cpuTime =\n",
        "        duration<double, milli>(endCpu - startCpu).count();\n",
        "\n",
        "    // GPU реализация\n",
        "\n",
        "    float* h_gpu = new float[N];\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_gpu[i] = h_data[i];\n",
        "    }\n",
        "\n",
        "    float* d_gpu;\n",
        "    cudaMalloc(&d_gpu, N * sizeof(float));\n",
        "    cudaMemcpy(d_gpu, h_gpu, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int gridSize = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    auto startGpu = high_resolution_clock::now();\n",
        "    gpuProcess<<<gridSize, BLOCK_SIZE>>>(d_gpu, 0, N);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto endGpu = high_resolution_clock::now();\n",
        "\n",
        "    double gpuTime =\n",
        "        duration<double, milli>(endGpu - startGpu).count();\n",
        "\n",
        "    cudaMemcpy(h_gpu, d_gpu, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Гибридная реализация\n",
        "\n",
        "    float* h_hybrid = new float[N];\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_hybrid[i] = h_data[i];\n",
        "    }\n",
        "\n",
        "    int cpuPart = N / 2;\n",
        "    int gpuPart = N - cpuPart;\n",
        "\n",
        "    auto startHybrid = high_resolution_clock::now();\n",
        "\n",
        "    // Обработка первой части массива на CPU\n",
        "    for (int i = 0; i < cpuPart; i++) {\n",
        "        h_hybrid[i] *= 2.0f;\n",
        "    }\n",
        "\n",
        "    // Копирование данных на GPU\n",
        "    cudaMemcpy(d_gpu, h_hybrid, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Обработка второй части массива на GPU\n",
        "    int hybridGridSize =\n",
        "        (gpuPart + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "\n",
        "    gpuProcess<<<hybridGridSize, BLOCK_SIZE>>>(d_gpu, cpuPart, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_hybrid, d_gpu, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    auto endHybrid = high_resolution_clock::now();\n",
        "    double hybridTime =\n",
        "        duration<double, milli>(endHybrid - startHybrid).count();\n",
        "\n",
        "    //Вывод результатов\n",
        "\n",
        "    cout << \"Размер массива: \" << N << \" элементов\\n\";\n",
        "    cout << \"CPU время: \" << cpuTime << \" мс\\n\";\n",
        "    cout << \"GPU время: \" << gpuTime << \" мс\\n\";\n",
        "    cout << \"Гибридное время: \" << hybridTime << \" мс\\n\";\n",
        "\n",
        "    // Освобождение памяти\n",
        "    cudaFree(d_gpu);\n",
        "    delete[] h_data;\n",
        "    delete[] h_cpu;\n",
        "    delete[] h_gpu;\n",
        "    delete[] h_hybrid;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vb3_c4LNAcW",
        "outputId": "9e30da37-4363-4eb7-e5da-bbeb2f841b1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc task3.cu -o task3\n",
        "!./task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOBjeR5WNKWS",
        "outputId": "a720a1dd-c6b6-4fa9-9341-e35429026e66"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер массива: 1000000 элементов\n",
            "CPU время: 2.67119 мс\n",
            "GPU время: 7.40906 мс\n",
            "Гибридное время: 2.64536 мс\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y mpich\n"
      ],
      "metadata": {
        "id": "Gv4_i_eyNVmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4_mpi.cpp\n",
        "#include <iostream>\n",
        "#include <mpi.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Размер массива\n",
        "const int N = 1'000'000;\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    // Инициализация MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank;\n",
        "    int size;\n",
        "\n",
        "    // Получение номера процесса и общего числа процессов\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Определение размера локального подмассива\n",
        "    int localSize = N / size;\n",
        "\n",
        "    // Выделение памяти под локальную часть массива\n",
        "    float* localData = new float[localSize];\n",
        "\n",
        "    // Инициализация локального массива\n",
        "    for (int i = 0; i < localSize; i++) {\n",
        "        localData[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // Синхронизация процессов перед началом измерений\n",
        "    MPI_Barrier(MPI_COMM_WORLD);\n",
        "    double startTime = MPI_Wtime();\n",
        "\n",
        "    // Локальная обработка массива\n",
        "    for (int i = 0; i < localSize; i++) {\n",
        "        localData[i] *= 2.0f;\n",
        "    }\n",
        "\n",
        "    // Буфер для итогового массива используется только процессом с rank 0\n",
        "    float* result = nullptr;\n",
        "    if (rank == 0) {\n",
        "        result = new float[N];\n",
        "    }\n",
        "\n",
        "    // Сбор локальных результатов в процесс 0\n",
        "    MPI_Gather(\n",
        "        localData,\n",
        "        localSize,\n",
        "        MPI_FLOAT,\n",
        "        result,\n",
        "        localSize,\n",
        "        MPI_FLOAT,\n",
        "        0,\n",
        "        MPI_COMM_WORLD\n",
        "    );\n",
        "\n",
        "    // Остановка таймера\n",
        "    double endTime = MPI_Wtime();\n",
        "\n",
        "    // Вывод времени выполнения только в корневом процессе\n",
        "    if (rank == 0) {\n",
        "        cout << \"Количество процессов: \" << size << endl;\n",
        "        cout << \"Время выполнения: \"\n",
        "             << (endTime - startTime) * 1000 << \" мс\\n\";\n",
        "    }\n",
        "\n",
        "    // Освобождение памяти\n",
        "    delete[] localData;\n",
        "    if (rank == 0) {\n",
        "        delete[] result;\n",
        "    }\n",
        "\n",
        "    // Завершение работы MPI\n",
        "    MPI_Finalize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILfP7kdgNadB",
        "outputId": "6ad50ab2-0f9b-4c57-ee69-42f3927ffbe9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task4_mpi.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root -np 2 ./task4_mpi\n",
        "!mpirun --allow-run-as-root -np 8 ./task4_mpi\n"
      ],
      "metadata": {
        "id": "LYl2g3rFNsTd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}