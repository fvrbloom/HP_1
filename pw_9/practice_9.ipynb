{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0-QrcQSToc-",
        "outputId": "9942293d-d55b-413e-b467-2006aeeb7770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mpi_mean_std.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile mpi_mean_std.cpp\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <cstdlib>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    vector<double> data;\n",
        "    vector<int> sendCounts(size), displs(size);\n",
        "\n",
        "    // Только процесс 0 создаёт массив\n",
        "    if (rank == 0) {\n",
        "        data.resize(N);\n",
        "        srand(42);\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            data[i] = static_cast<double>(rand()) / RAND_MAX;\n",
        "        }\n",
        "\n",
        "        int base = N / size;\n",
        "        int remainder = N % size;\n",
        "\n",
        "        int offset = 0;\n",
        "        for (int i = 0; i < size; i++) {\n",
        "            sendCounts[i] = base + (i < remainder ? 1 : 0);\n",
        "            displs[i] = offset;\n",
        "            offset += sendCounts[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Рассылка размеров частей\n",
        "    int localSize;\n",
        "    MPI_Scatter(sendCounts.data(), 1, MPI_INT,\n",
        "                &localSize, 1, MPI_INT,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    vector<double> localData(localSize);\n",
        "\n",
        "    // Распределение массива\n",
        "    MPI_Scatterv(data.data(), sendCounts.data(), displs.data(), MPI_DOUBLE,\n",
        "                 localData.data(), localSize, MPI_DOUBLE,\n",
        "                 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Локальные вычисления\n",
        "    double localSum = 0.0;\n",
        "    double localSumSquares = 0.0;\n",
        "\n",
        "    for (double x : localData) {\n",
        "        localSum += x;\n",
        "        localSumSquares += x * x;\n",
        "    }\n",
        "\n",
        "    // Глобальные суммы\n",
        "    double globalSum = 0.0;\n",
        "    double globalSumSquares = 0.0;\n",
        "\n",
        "    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&localSumSquares, &globalSumSquares, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Вычисление результата\n",
        "    if (rank == 0) {\n",
        "        double mean = globalSum / N;\n",
        "        double stddev = sqrt(globalSumSquares / N - mean * mean);\n",
        "\n",
        "        cout << \"Среднее значение: \" << mean << endl;\n",
        "        cout << \"Стандартное отклонение: \" << stddev << endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_mean_std\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uFUpgSJT7qk",
        "outputId": "b09a329f-e0ba-4d40-e09e-9d8a22ee7eeb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Среднее значение: 0.500147\n",
            "Стандартное отклонение: 0.288672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_gauss.cpp\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Размер системы (можно менять)\n",
        "    const int N = 4;\n",
        "\n",
        "    vector<double> A;\n",
        "    vector<double> b;\n",
        "\n",
        "    int rowsPerProc = N / size;\n",
        "\n",
        "    vector<double> localA(rowsPerProc * N);\n",
        "    vector<double> localB(rowsPerProc);\n",
        "\n",
        "    // Инициализация только на rank = 0\n",
        "    if (rank == 0) {\n",
        "        A = {\n",
        "            10, 2, 3, 4,\n",
        "             3,10, 4, 5,\n",
        "             1, 2,10, 6,\n",
        "             2, 3, 4,10\n",
        "        };\n",
        "\n",
        "        b = {16, 22, 19, 29};\n",
        "    }\n",
        "\n",
        "    // Распределение строк матрицы и вектора\n",
        "    MPI_Scatter(A.data(), rowsPerProc * N, MPI_DOUBLE,\n",
        "                localA.data(), rowsPerProc * N, MPI_DOUBLE,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    MPI_Scatter(b.data(), rowsPerProc, MPI_DOUBLE,\n",
        "                localB.data(), rowsPerProc, MPI_DOUBLE,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    vector<double> pivotRow(N);\n",
        "    double pivotB;\n",
        "\n",
        "    // Прямой ход\n",
        "    for (int k = 0; k < N; k++) {\n",
        "        int owner = k / rowsPerProc;\n",
        "\n",
        "        if (rank == owner) {\n",
        "            int localRow = k % rowsPerProc;\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                pivotRow[j] = localA[localRow * N + j];\n",
        "            }\n",
        "            pivotB = localB[localRow];\n",
        "        }\n",
        "\n",
        "        MPI_Bcast(pivotRow.data(), N, MPI_DOUBLE, owner, MPI_COMM_WORLD);\n",
        "        MPI_Bcast(&pivotB, 1, MPI_DOUBLE, owner, MPI_COMM_WORLD);\n",
        "\n",
        "        for (int i = 0; i < rowsPerProc; i++) {\n",
        "            int globalRow = rank * rowsPerProc + i;\n",
        "            if (globalRow > k) {\n",
        "                double factor = localA[i * N + k] / pivotRow[k];\n",
        "                for (int j = k; j < N; j++) {\n",
        "                    localA[i * N + j] -= factor * pivotRow[j];\n",
        "                }\n",
        "                localB[i] -= factor * pivotB;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Сбор матрицы и вектора на rank = 0\n",
        "    MPI_Gather(localA.data(), rowsPerProc * N, MPI_DOUBLE,\n",
        "               A.data(), rowsPerProc * N, MPI_DOUBLE,\n",
        "               0, MPI_COMM_WORLD);\n",
        "\n",
        "    MPI_Gather(localB.data(), rowsPerProc, MPI_DOUBLE,\n",
        "               b.data(), rowsPerProc, MPI_DOUBLE,\n",
        "               0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Обратный ход (только rank = 0)\n",
        "    if (rank == 0) {\n",
        "        vector<double> x(N);\n",
        "\n",
        "        for (int i = N - 1; i >= 0; i--) {\n",
        "            x[i] = b[i];\n",
        "            for (int j = i + 1; j < N; j++) {\n",
        "                x[i] -= A[i * N + j] * x[j];\n",
        "            }\n",
        "            x[i] /= A[i * N + i];\n",
        "        }\n",
        "\n",
        "        cout << \"Решение системы:\" << endl;\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            cout << \"x[\" << i << \"] = \" << x[i] << endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMyc4o8KUwOX",
        "outputId": "d5d858bc-fd7d-49cf-c7d5-b3cd3e8ab171"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_gauss.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_gauss.cpp -O2 -o mpi_gauss\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_gauss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFbSpr0HU75r",
        "outputId": "e05cbea0-9762-4f48-a49b-28d34d2b0ca7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Решение системы:\n",
            "x[0] = 0.382895\n",
            "x[1] = 0.744966\n",
            "x[2] = 0.200998\n",
            "x[3] = 2.51953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_floyd.cpp\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <algorithm>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "const int INF = 1000000000;\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Размер графа (можно менять)\n",
        "    const int N = 4;\n",
        "\n",
        "    vector<int> G;\n",
        "    vector<int> localG;\n",
        "\n",
        "    int rowsPerProc = N / size;\n",
        "\n",
        "    // Инициализация графа только на rank = 0\n",
        "    if (rank == 0) {\n",
        "        G = {\n",
        "            0,   3,   INF, 7,\n",
        "            8,   0,   2,   INF,\n",
        "            5,   INF, 0,   1,\n",
        "            2,   INF, INF, 0\n",
        "        };\n",
        "    }\n",
        "\n",
        "    localG.resize(rowsPerProc * N);\n",
        "\n",
        "    // Распределение строк матрицы\n",
        "    MPI_Scatter(G.data(), rowsPerProc * N, MPI_INT,\n",
        "                localG.data(), rowsPerProc * N, MPI_INT,\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    vector<int> fullG(N * N);\n",
        "\n",
        "    // Алгоритм Флойда–Уоршелла\n",
        "    for (int k = 0; k < N; k++) {\n",
        "        // Собираем обновлённую матрицу у всех процессов\n",
        "        MPI_Allgather(localG.data(), rowsPerProc * N, MPI_INT,\n",
        "                      fullG.data(), rowsPerProc * N, MPI_INT,\n",
        "                      MPI_COMM_WORLD);\n",
        "\n",
        "        // Обновление локальных строк\n",
        "        for (int i = 0; i < rowsPerProc; i++) {\n",
        "            int globalI = rank * rowsPerProc + i;\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                int throughK = fullG[globalI * N + k] + fullG[k * N + j];\n",
        "                localG[i * N + j] = min(localG[i * N + j], throughK);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Сбор финальной матрицы на rank = 0\n",
        "    MPI_Gather(localG.data(), rowsPerProc * N, MPI_INT,\n",
        "               G.data(), rowsPerProc * N, MPI_INT,\n",
        "               0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Вывод результата\n",
        "    if (rank == 0) {\n",
        "        cout << \"Матрица кратчайших расстояний:\" << endl;\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            for (int j = 0; j < N; j++) {\n",
        "                if (G[i * N + j] >= INF)\n",
        "                    cout << \"INF \";\n",
        "                else\n",
        "                    cout << G[i * N + j] << \" \";\n",
        "            }\n",
        "            cout << endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQALYqnyVIDk",
        "outputId": "5187dbf6-995a-4110-9086-4239776f8745"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_floyd.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_floyd.cpp -O2 -o mpi_floyd\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_floyd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZpq_PjAVoJt",
        "outputId": "106e0872-ec0f-4d61-a35f-90e97e34eaec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица кратчайших расстояний:\n",
            "0 3 5 6 \n",
            "5 0 2 3 \n",
            "3 6 0 1 \n",
            "2 5 7 0 \n"
          ]
        }
      ]
    }
  ]
}