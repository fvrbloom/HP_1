{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fd1Md2AYdaA",
        "outputId": "20f52513-a825-4007-f6b5-823fcfb8bad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting openmp_stats.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile openmp_stats.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <omp.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main() {\n",
        "    const int N = 10'000'000;\n",
        "    vector<double> data(N);\n",
        "\n",
        "    // Последовательная часть: инициализация массива\n",
        "    double t_start = omp_get_wtime();\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        data[i] = 1.0;\n",
        "    }\n",
        "    double t_init = omp_get_wtime();\n",
        "\n",
        "    double sum = 0.0;\n",
        "    double sumSquares = 0.0;\n",
        "\n",
        "    // Параллельная часть\n",
        "    double t_parallel_start = omp_get_wtime();\n",
        "\n",
        "    #pragma omp parallel for reduction(+:sum, sumSquares)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        sum += data[i];\n",
        "        sumSquares += data[i] * data[i];\n",
        "    }\n",
        "\n",
        "    double t_parallel_end = omp_get_wtime();\n",
        "\n",
        "    // Последовательная часть: финальные вычисления\n",
        "    double mean = sum / N;\n",
        "    double variance = sumSquares / N - mean * mean;\n",
        "    double t_end = omp_get_wtime();\n",
        "\n",
        "    cout << \"Сумма: \" << sum << endl;\n",
        "    cout << \"Среднее: \" << mean << endl;\n",
        "    cout << \"Дисперсия: \" << variance << endl;\n",
        "\n",
        "    cout << \"\\nВремя инициализации (последовательно): \"\n",
        "         << (t_init - t_start) << \" сек\" << endl;\n",
        "\n",
        "    cout << \"Время параллельных вычислений: \"\n",
        "         << (t_parallel_end - t_parallel_start) << \" сек\" << endl;\n",
        "\n",
        "    cout << \"Общее время выполнения: \"\n",
        "         << (t_end - t_start) << \" сек\" << endl;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -fopenmp -O2 openmp_stats.cpp -o openmp_stats\n"
      ],
      "metadata": {
        "id": "NPJfGgGVYu0r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./openmp_stats\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjAJFRsTZBxP",
        "outputId": "3d0c313f-0a6d-4ac1-cf5f-b1c2b3f70c11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сумма: 1e+07\n",
            "Среднее: 1\n",
            "Дисперсия: 0\n",
            "\n",
            "Время инициализации (последовательно): 0.00864899 сек\n",
            "Время параллельных вычислений: 0.0145394 сек\n",
            "Общее время выполнения: 0.0231888 сек\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda_memory_access.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "const int N = 1 << 20;        // ~1 000 000 элементов\n",
        "const int BLOCK_SIZE = 256;\n",
        "\n",
        "// ----------------------------\n",
        "// 1. Коалесцированный доступ\n",
        "// ----------------------------\n",
        "__global__ void coalescedKernel(float* data) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < N) {\n",
        "        data[idx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// --------------------------------\n",
        "// 2. Некоалесцированный доступ\n",
        "// --------------------------------\n",
        "__global__ void nonCoalescedKernel(float* data) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    if (idx < N) {\n",
        "        int badIndex = (idx * 32) % N;\n",
        "        data[badIndex] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "// ------------------------------------\n",
        "// 3. Коалесцированный + shared memory\n",
        "// ------------------------------------\n",
        "__global__ void sharedMemoryKernel(float* data) {\n",
        "    __shared__ float buffer[BLOCK_SIZE];\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (idx < N) {\n",
        "        buffer[tid] = data[idx];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (idx < N) {\n",
        "        buffer[tid] *= 2.0f;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (idx < N) {\n",
        "        data[idx] = buffer[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "// ----------------------------\n",
        "// Замер времени\n",
        "// ----------------------------\n",
        "float measureKernel(void (*kernel)(float*), float* d_data, dim3 grid, dim3 block) {\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    kernel<<<grid, block>>>(d_data);\n",
        "    cudaEventRecord(stop);\n",
        "\n",
        "    cudaEventSynchronize(stop);\n",
        "    float time;\n",
        "    cudaEventElapsedTime(&time, start, stop);\n",
        "\n",
        "    return time;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float* h_data = new float[N];\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_data[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    float* d_data;\n",
        "    cudaMalloc(&d_data, N * sizeof(float));\n",
        "    cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 block(BLOCK_SIZE);\n",
        "    dim3 grid((N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    float t1 = measureKernel(coalescedKernel, d_data, grid, block);\n",
        "    float t2 = measureKernel(nonCoalescedKernel, d_data, grid, block);\n",
        "    float t3 = measureKernel(sharedMemoryKernel, d_data, grid, block);\n",
        "\n",
        "    cout << \"Коалесцированный доступ: \" << t1 << \" мс\" << endl;\n",
        "    cout << \"Некoалесцированный доступ: \" << t2 << \" мс\" << endl;\n",
        "    cout << \"Shared memory: \" << t3 << \" мс\" << endl;\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    delete[] h_data;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL-KKCswZWIo",
        "outputId": "a5250e2b-9069-45a7-8c99-557b6ddbb16a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_memory_access.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 cuda_memory_access.cu -o mem_access\n",
        "!./mem_access\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2LY2aUzZbKM",
        "outputId": "999b2a89-7d67-4182-c4d0-76966bd3a815"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mcuda_memory_access.cu(24)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"stride\"\u001b[0m was declared but never referenced\n",
            "      int stride = blockDim.x * gridDim.x;\n",
            "          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "Коалесцированный доступ: 0.16256 мс\n",
            "Некoалесцированный доступ: 0.284832 мс\n",
            "Shared memory: 0.046016 мс\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid_profiling.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <omp.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "const int N = 1 << 20;       // ~1 000 000 элементов\n",
        "const int HALF = N / 2;\n",
        "const int BLOCK_SIZE = 256;\n",
        "\n",
        "// CUDA-ядро\n",
        "__global__ void gpuKernel(float* data, int offset, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        data[offset + idx] *= 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float* h_data;\n",
        "    cudaMallocHost(&h_data, N * sizeof(float)); // pinned memory\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_data[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    float* d_data;\n",
        "    cudaMalloc(&d_data, N * sizeof(float));\n",
        "\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    double startTotal = omp_get_wtime();\n",
        "\n",
        "    // Асинхронная передача CPU → GPU\n",
        "    double t_copy_start = omp_get_wtime();\n",
        "    cudaMemcpyAsync(d_data + HALF,\n",
        "                    h_data + HALF,\n",
        "                    HALF * sizeof(float),\n",
        "                    cudaMemcpyHostToDevice,\n",
        "                    stream);\n",
        "    double t_copy_end = omp_get_wtime();\n",
        "\n",
        "    // Параллельная обработка CPU\n",
        "    double t_cpu_start = omp_get_wtime();\n",
        "    #pragma omp parallel for\n",
        "    for (int i = 0; i < HALF; i++) {\n",
        "        h_data[i] *= 2.0f;\n",
        "    }\n",
        "    double t_cpu_end = omp_get_wtime();\n",
        "\n",
        "    // Запуск CUDA-ядра\n",
        "    int gridSize = (HALF + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    double t_gpu_start = omp_get_wtime();\n",
        "    gpuKernel<<<gridSize, BLOCK_SIZE, 0, stream>>>(d_data, HALF, HALF);\n",
        "    cudaStreamSynchronize(stream);\n",
        "    double t_gpu_end = omp_get_wtime();\n",
        "\n",
        "    // Асинхронная передача GPU → CPU\n",
        "    cudaMemcpyAsync(h_data + HALF,\n",
        "                    d_data + HALF,\n",
        "                    HALF * sizeof(float),\n",
        "                    cudaMemcpyDeviceToHost,\n",
        "                    stream);\n",
        "    cudaStreamSynchronize(stream);\n",
        "\n",
        "    double endTotal = omp_get_wtime();\n",
        "\n",
        "    cout << \"Время передачи CPU → GPU: \"\n",
        "         << (t_copy_end - t_copy_start) << \" сек\" << endl;\n",
        "\n",
        "    cout << \"Время CPU вычислений: \"\n",
        "         << (t_cpu_end - t_cpu_start) << \" сек\" << endl;\n",
        "\n",
        "    cout << \"Время GPU вычислений: \"\n",
        "         << (t_gpu_end - t_gpu_start) << \" сек\" << endl;\n",
        "\n",
        "    cout << \"Общее время выполнения: \"\n",
        "         << (endTotal - startTotal) << \" сек\" << endl;\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    cudaFreeHost(h_data);\n",
        "    cudaStreamDestroy(stream);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz6isD0RZ6tg",
        "outputId": "f0ed261f-0b72-4e63-e0c9-f1e169cc6d0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hybrid_profiling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -Xcompiler -fopenmp hybrid_profiling.cu -o hybrid_prof\n",
        "!./hybrid_prof\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FHBa6WNZ--y",
        "outputId": "b74ae39f-1d76-433c-e56a-6f0de2a0f834"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Время передачи CPU → GPU: 2.0473e-05 сек\n",
            "Время CPU вычислений: 0.00130168 сек\n",
            "Время GPU вычислений: 0.000162436 сек\n",
            "Общее время выполнения: 0.00167191 сек\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mpi_scaling.cpp\n",
        "#include <mpi.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <limits>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Размер массива для strong scaling\n",
        "    const int N = 1'000'000;\n",
        "\n",
        "    int localN = N / size;\n",
        "\n",
        "    vector<double> localData(localN);\n",
        "\n",
        "    // Инициализация локальных данных\n",
        "    for (int i = 0; i < localN; i++) {\n",
        "        localData[i] = 1.0;\n",
        "    }\n",
        "\n",
        "    double t_start = MPI_Wtime();\n",
        "\n",
        "    // Локальные агрегаты\n",
        "    double localSum = 0.0;\n",
        "    double localMin = numeric_limits<double>::max();\n",
        "    double localMax = numeric_limits<double>::lowest();\n",
        "\n",
        "    for (double x : localData) {\n",
        "        localSum += x;\n",
        "        localMin = min(localMin, x);\n",
        "        localMax = max(localMax, x);\n",
        "    }\n",
        "\n",
        "    // Глобальные агрегаты\n",
        "    double globalSum, globalMin, globalMax;\n",
        "\n",
        "    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&localMin, &globalMin, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n",
        "    MPI_Reduce(&localMax, &globalMax, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    double t_end = MPI_Wtime();\n",
        "\n",
        "    if (rank == 0) {\n",
        "        cout << \"Процессов: \" << size << endl;\n",
        "        cout << \"Сумма: \" << globalSum << endl;\n",
        "        cout << \"Мин: \" << globalMin << endl;\n",
        "        cout << \"Макс: \" << globalMax << endl;\n",
        "        cout << \"Время выполнения: \"\n",
        "             << (t_end - t_start) << \" сек\\n\" << endl;\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kYzyXyaaWmk",
        "outputId": "c6be60b3-0d83-4f2e-cf58-f05fd19675cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mpi_scaling.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpic++ mpi_scaling.cpp -O2 -o mpi_scaling\n"
      ],
      "metadata": {
        "id": "KR5rCnh0abXB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -np 1 ./mpi_scaling\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_scaling\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_scaling\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 8 ./mpi_scaling\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMIOIGBYadzH",
        "outputId": "306cf370-3fb4-493e-9998-b25ae5974e25"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Процессов: 1\n",
            "Сумма: 1e+06\n",
            "Мин: 1\n",
            "Макс: 1\n",
            "Время выполнения: 0.00161841 сек\n",
            "\n",
            "Процессов: 2\n",
            "Сумма: 1e+06\n",
            "Мин: 1\n",
            "Макс: 1\n",
            "Время выполнения: 0.00077528 сек\n",
            "\n",
            "Процессов: 4\n",
            "Сумма: 1e+06\n",
            "Мин: 1\n",
            "Макс: 1\n",
            "Время выполнения: 0.00105961 сек\n",
            "\n",
            "Процессов: 8\n",
            "Сумма: 1e+06\n",
            "Мин: 1\n",
            "Макс: 1\n",
            "Время выполнения: 0.00553872 сек\n",
            "\n"
          ]
        }
      ]
    }
  ]
}